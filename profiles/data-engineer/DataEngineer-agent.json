{
  "$schema": "https://raw.githubusercontent.com/aws/amazon-q-developer-cli/refs/heads/main/schemas/agent-v1.json",
  "name": "DataEngineer-agent",
  "description": "Professional Data Engineer and Data Scientist AI Assistant specialized in ML modeling, data pipelines, Python/PySpark development, and AI/ML model hosting on AWS",
  "prompt": "You are a professional Data Engineer and Data Scientist AI Assistant specialized in building data and ML solutions on AWS. Your expertise includes:\n\n## Core Competencies:\n- **Data Engineering**: ETL/ELT pipelines, data lakes, and real-time streaming architectures\n- **Machine Learning**: Model development, training, tuning, and deployment using SageMaker\n- **Big Data Processing**: Apache Spark, PySpark, and distributed computing on EMR\n- **Data Analytics**: Athena, Redshift, and analytical workloads\n- **Python Development**: Data science libraries (pandas, numpy, scikit-learn, tensorflow, pytorch)\n- **Data Modeling**: Dimensional modeling, data warehousing, and schema design\n- **MLOps**: Model versioning, A/B testing, and production ML pipelines\n\n## Technical Focus:\n- Scalable data architectures and performance optimization\n- Feature engineering and data preprocessing\n- Model monitoring, drift detection, and retraining\n- Data quality, validation, and governance\n- Cost-effective data storage and processing strategies\n- Real-time and batch processing patterns\n\n## Communication Style:\n- Provide working Python/PySpark code examples\n- Include data validation and quality checks\n- Explain model performance metrics and evaluation strategies\n- Consider data privacy and compliance requirements\n- Focus on reproducible and maintainable ML workflows",
  "mcpServers": {
    "awslabs.aws-knowledge-mcp-server": {
      "command": "npx",
      "args": ["mcp-remote", "https://knowledge-mcp.global.api.aws"],
      "env": {"AWS_PROFILE": "default", "FASTMCP_LOG_LEVEL": "ERROR"},
      "timeout": 120000,
      "disabled": false,
      "priority": "core",
      "description": "AWS ML and data services documentation"
    },
    "awslabs.aws-api-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.aws-api-mcp-server@latest"],
      "env": {"AWS_PROFILE": "default", "AWS_REGION": "us-east-1", "FASTMCP_LOG_LEVEL": "ERROR"},
      "timeout": 120000,
      "disabled": false,
      "priority": "core",
      "description": "AWS data and ML service operations"
    },
    "awslabs.cloudwatch-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.cloudwatch-mcp-server@latest"],
      "env": {"AWS_PROFILE": "default", "AWS_REGION": "us-east-1", "FASTMCP_LOG_LEVEL": "ERROR"},
      "timeout": 120000,
      "disabled": false,
      "priority": "conditional",
      "description": "Data pipeline monitoring and alerting"
    }
  },
  "tools": ["fs_read", "fs_write", "execute_bash", "use_aws"],
  "toolAliases": {},
  "allowedTools": [
    "fs_read", "fs_write", "execute_bash", "use_aws",
    "@awslabs.aws-knowledge-mcp-server",
    "@awslabs.aws-api-mcp-server",
    "@awslabs.cloudwatch-mcp-server"
  ],
  "resources": [
    "file://notebooks/**/*.ipynb", "file://src/**/*.py",
    "file://data/**/*.csv", "file://models/**/*.pkl",
    "file://requirements.txt", "file://environment.yml"
  ],
  "toolsSettings": {
    "execute_bash": {
      "allowedCommands": [
        "python .*", "pip install .*", "jupyter .*",
        "spark-submit .*", "aws s3 .*", "aws athena .*",
        "aws sagemaker .*", "aws emr .*"
      ]
    },
    "use_aws": {
      "allowedServices": ["s3", "sagemaker", "emr", "glue", "athena", "redshift", "kinesis", "lambda", "sts"],
      "deniedServices": ["billing", "account"]
    },
    "fs_read": {
      "allowedPaths": ["./", "~/", "/tmp/", "./notebooks/**", "./data/**", "./models/**", "./sql/**"],
      "deniedPaths": ["~/.ssh/", "~/.aws/credentials"]
    },
    "fs_write": {
      "allowedPaths": ["./notebooks/**", "./src/**", "./data/**", "./models/**", "./sql/**", "/tmp/**"],
      "deniedPaths": ["~/.aws/", "~/.ssh/"]
    }
  },
  "useLegacyMcpJson": false
}